{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-path PATH] [-market MARKET]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/liupeng/Library/Jupyter/runtime/kernel-v2-10460fYMx4InVpaSJ.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#  读取股票数据，生成特征\n",
    "class EOD_Preprocessor:\n",
    "    def __init__(self, data_path, market_name):\n",
    "        self.data_path = data_path # /Users/liupeng/Desktop/DSAA 5020 Final Project/TGCN_with_latest_data/2023.12.14(Data acquisition& Preprocessing)\n",
    "        self.date_format = '%Y-%m-%d' \n",
    "        self.market_name = market_name # S&P500\n",
    "\n",
    "    # def _read_EOD_data(self):\n",
    "    #     self.data_EOD = []\n",
    "    #     for index, ticker in enumerate(self.tickers):\n",
    "    #         single_EOD = np.genfromtxt(\n",
    "    #             os.path.join(self.data_path,\"S&P500_original_data\", ticker +\n",
    "    #                          '_stock_data.csv'), dtype=str, delimiter=',',\n",
    "    #             skip_header=True\n",
    "    #         )\n",
    "    #         self.data_EOD.append(single_EOD)\n",
    "    #         # if index > 99:\n",
    "    #         #     break\n",
    "    #     print('#stocks\\' EOD data readin:', len(self.data_EOD))\n",
    "    #     assert len(self.tickers) == len(self.data_EOD), 'length of tickers ' \\\n",
    "    #                                                     'and stocks not match'\n",
    "\n",
    "    def _read_EOD_data(self):\n",
    "        self.data_EOD = []\n",
    "        for ticker in self.tickers:\n",
    "            file_path = os.path.join(self.data_path, \"S&P500_original_data\", f\"{ticker}_stock_data.csv\")\n",
    "            single_EOD = pd.read_csv(file_path)\n",
    "            self.data_EOD.append(single_EOD.values)\n",
    "        print('#stocks\\' EOD data readin:', len(self.data_EOD))\n",
    "        assert len(self.tickers) == len(self.data_EOD), 'length of tickers and stocks not match'\n",
    "\n",
    "    # def _read_tickers(self, ticker_fname):\n",
    "    #     self.tickers = np.genfromtxt(ticker_fname, dtype=str, delimiter='\\t',\n",
    "    #                                  skip_header=True)[:, 0]\n",
    "    def _read_tickers(self):\n",
    "        tickers_path = os.path.join(self.data_path, \"Relation_Data(478)\", \"sp500_tickers_478.csv\")\n",
    "        self.tickers = pd.read_csv(tickers_path)['Ticker'].tolist()\n",
    "\n",
    "    def _transfer_EOD_str(self, selected_EOD_str, tra_date_index):\n",
    "        selected_EOD = np.zeros(selected_EOD_str.shape, dtype=float)\n",
    "        for row, daily_EOD in enumerate(selected_EOD_str):\n",
    "            # print('daily_EOD:', daily_EOD)\n",
    "            date_str = daily_EOD[0]\n",
    "            # date_str = date_str.replace('-04:00', '')\n",
    "            # print('date_str:', date_str)\n",
    "            # print('tra_date_index:', tra_date_index)\n",
    "            selected_EOD[row][0] = tra_date_index[date_str]\n",
    "            # print('selected_EOD[row][0]:', selected_EOD[row][0])\n",
    "            for col in range(1, selected_EOD_str.shape[1]):\n",
    "                selected_EOD[row][col] = float(daily_EOD[col])\n",
    "        return selected_EOD\n",
    "\n",
    "    '''\n",
    "        Transform the original EOD data collected from Google Finance to a\n",
    "        friendly format to fit machine learning model via the following steps:\n",
    "            Calculate moving average (5-days, 10-days, 20-days, 30-days),\n",
    "            ignoring suspension days (market open, only suspend this stock)\n",
    "            Normalize features by (feature - min) / (max - min)\n",
    "    '''\n",
    "    def generate_feature(self, selected_tickers_fname, begin_date, opath,\n",
    "                         return_days=1, pad_begin=29):\n",
    "        # 新的交易日期文件路径\n",
    "        trading_dates_path = os.path.join(self.data_path, 'S&P500_aver_line_dates.csv')\n",
    "        trading_dates = pd.read_csv(trading_dates_path)['Date'].tolist()\n",
    "\n",
    "        # trading_dates = np.genfromtxt(\n",
    "        #     os.path.join(self.data_path,self.market_name + '_aver_line_dates_latest.csv'),\n",
    "        #     dtype=str, delimiter=',', skip_header=False\n",
    "        # )\n",
    "        # trading_dates = trading_dates[29:]  # offset for the first 30-days average\n",
    "        print('#trading dates:', len(trading_dates)) # trading dates是什么？为什么文件中显示是股票ticker的数量？ trading_dates：1275\n",
    "        # begin_date = datetime.strptime(trading_dates[29], self.date_format)\n",
    "        print('begin date:', begin_date)\n",
    "        # transform the trading dates into a dictionary with index\n",
    "        index_tra_dates = {}\n",
    "        tra_dates_index = {}\n",
    "        for index, date in enumerate(trading_dates):\n",
    "            tra_dates_index[date] = index\n",
    "            index_tra_dates[index] = date\n",
    "        # read in tickers file: NASDAQ_tickers_qualify_dr-0.98_min-5_smooth.csv\n",
    "        self.tickers = np.genfromtxt(\n",
    "            os.path.join(self.data_path, selected_tickers_fname),\n",
    "            dtype=str, delimiter='\\t', skip_header=False\n",
    "        )\n",
    "        print('#tickers selected:', len(self.tickers))\n",
    "        self._read_EOD_data()\n",
    "        for stock_index, single_EOD in enumerate(self.data_EOD):\n",
    "            # select data within the begin_date\n",
    "            # begin_date_row = -1\n",
    "            for date_index, daily_EOD in enumerate(single_EOD):\n",
    "                date_str = daily_EOD[0]\n",
    "                # date_str = date_str.replace('-04:00', '') #这部分代码为什么在_transfer_EOD_str中又被执行了一遍？\n",
    "                cur_date = datetime.strptime(date_str, self.date_format)\n",
    "                # if cur_date > begin_date:\n",
    "                    # print('cur_date:', cur_date)\n",
    "                    # begin_date_row = date_index\n",
    "                    # print(single_EOD[date_index,:])\n",
    "                    # break\n",
    "            selected_EOD_str = single_EOD\n",
    "            selected_EOD = self._transfer_EOD_str(selected_EOD_str,\n",
    "                                                  tra_dates_index)\n",
    "            # Question：数据形状对不上：selected_EOD_str.shape = (1274, 6), selected_EOD.shape = (1274, 6)\n",
    "            # calculate moving average features\n",
    "            begin_date_row = 29\n",
    "            # for row in selected_EOD[:, 0]:\n",
    "            #     row = int(row)\n",
    "            #     if row >= pad_begin:   # offset for the first 30-days average\n",
    "            #         begin_date_row = row\n",
    "            #         break\n",
    "            mov_aver_features = np.zeros(\n",
    "                [selected_EOD.shape[0], 4], dtype=float\n",
    "            )   # 4 columns refers to 5-, 10-, 20-, 30-days average\n",
    "            for row in range(begin_date_row, selected_EOD.shape[0]):\n",
    "                date_index = selected_EOD[row][0]\n",
    "                aver_5 = 0.0\n",
    "                aver_10 = 0.0\n",
    "                aver_20 = 0.0\n",
    "                aver_30 = 0.0\n",
    "                count_5 = 0\n",
    "                count_10 = 0\n",
    "                count_20 = 0\n",
    "                count_30 = 0\n",
    "                for offset in range(30):\n",
    "                    date_gap = date_index - selected_EOD[row - offset][0]\n",
    "                    if date_gap < 5:\n",
    "                        count_5 += 1\n",
    "                        aver_5 += selected_EOD[row - offset][4]\n",
    "                    if date_gap < 10:\n",
    "                        count_10 += 1\n",
    "                        aver_10 += selected_EOD[row - offset][4]\n",
    "                    if date_gap < 20:\n",
    "                        count_20 += 1\n",
    "                        aver_20 += selected_EOD[row - offset][4]\n",
    "                    if date_gap < 30:\n",
    "                        count_30 += 1\n",
    "                        aver_30 += selected_EOD[row - offset][4]\n",
    "                mov_aver_features[row][0] = aver_5 / count_5\n",
    "                mov_aver_features[row][1] = aver_10 / count_10\n",
    "                mov_aver_features[row][2] = aver_20 / count_20\n",
    "                mov_aver_features[row][3] = aver_30 / count_30\n",
    "\n",
    "            '''\n",
    "                normalize features by feature / max, the max price is the\n",
    "                max of close prices, I give up to subtract min for easier\n",
    "                return ratio calculation.\n",
    "            '''\n",
    "            pri_min = np.min(selected_EOD[:, 5]) # 收盘价的最小值\n",
    "            price_max = np.max(selected_EOD[:, 5])# 收盘价的最大值\n",
    "            print(self.tickers[stock_index], 'minimum:', pri_min,\n",
    "                  'maximum:', price_max, 'ratio:', price_max / pri_min)\n",
    "            if price_max / pri_min > 10:      # 价格最大值与最小值的比值大于10，说明这个股票的价格波动很大，不适合用来做预测 ?\n",
    "                print('!!!!!!!!!')\n",
    "            # open_high_low = (selected_EOD[:, 1:4] - price_min) / \\\n",
    "            #                 (price_max - price_min)\n",
    "            mov_aver_features = mov_aver_features / price_max  #移动平均值除以收盘价最大值进行标准化是否合理？\n",
    "\n",
    "            '''\n",
    "                generate feature and ground truth in the following format:\n",
    "                date_index, 5-day, 10-day, 20-day, 30-day, close price\n",
    "                two ways to pad missing dates:\n",
    "                for dates without record, pad a row [date_index, -1234 * 5]\n",
    "            '''\n",
    "            features = np.ones([len(trading_dates) - pad_begin, 6],\n",
    "                               dtype=float) * -1234\n",
    "            # features：(1275-29=1246, 6)\n",
    "            # data missed at the beginning\n",
    "            for row in range(len(trading_dates) - pad_begin):\n",
    "                features[row][0] = row\n",
    "            for row in range(begin_date_row, selected_EOD.shape[0]):\n",
    "                cur_index = int(selected_EOD[row][0])\n",
    "                features[cur_index - pad_begin][1:5] = mov_aver_features[row]\n",
    "                # 这个if语句的作用是什么？有什么意义？\n",
    "                # if cur_index - int(selected_EOD[row - return_days][0]) == return_days:\n",
    "                features[cur_index - pad_begin][-1] = selected_EOD[row][4] / price_max\n",
    "\n",
    "            # write out\n",
    "            np.savetxt(os.path.join(opath, self.market_name + '_' +\n",
    "                                    self.tickers[stock_index] + '_' +\n",
    "                                    str(return_days) + '.csv'), features,\n",
    "                       fmt='%.6f', delimiter=',')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    desc = \"pre-process EOD data market by market, including listing all \" \\\n",
    "           \"trading days, all satisfied stocks (5 years & high price), \" \\\n",
    "           \"normalizing and compensating data\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "    parser.add_argument('-path', help='path of EOD data')\n",
    "    parser.add_argument('-market', help='market name')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.path is None:\n",
    "        args.path = '/Users/liupeng/Desktop/DSAA 5020 Final Project/TGCN_with_latest_data/2023.12.14(Data acquisition& Preprocessing)'\n",
    "    if args.market is None:\n",
    "        args.market = 'S&P500'\n",
    "\n",
    "    processor = EOD_Preprocessor(args.path, args.market)\n",
    "    processor.generate_feature(\n",
    "        'sp500_tickers_478.csv',  # 这里假设传递的是正确的tickers文件名\n",
    "        datetime.strptime('2015-01-02', '%Y-%m-%d'),\n",
    "        os.path.join(args.path, 'SP500_EOD'),  # 假设输出路径为Processed_Data子文件夹\n",
    "        return_days=1,\n",
    "        pad_begin=29\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/liupeng/Desktop/DSAA 5020 Final Project/TGCN_with_latest_data/2023.12.14(Data acquisition& Preprocessing)/Data_processing_code'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#trading dates: 2244\n",
      "begin date: 2015-01-02 00:00:00\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "/Users/liupeng/Desktop/DSAA 5020 Final Project/TGCN_with_latest_data/2023.12.14(Data acquisition& Preprocessing)/sp500_tickers_478.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 180\u001b[0m\n\u001b[1;32m    177\u001b[0m processor \u001b[38;5;241m=\u001b[39m EOD_Preprocessor(data_path, market_name)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# 调用generate_feature方法处理数据\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msp500_tickers_478.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2015-01-02\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSP500_EOD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_begin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m29\u001b[39;49m\n\u001b[1;32m    186\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 71\u001b[0m, in \u001b[0;36mEOD_Preprocessor.generate_feature\u001b[0;34m(self, selected_tickers_fname, begin_date, opath, return_days, pad_begin)\u001b[0m\n\u001b[1;32m     69\u001b[0m     index_tra_dates[index] \u001b[38;5;241m=\u001b[39m date\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# read in tickers file: NASDAQ_tickers_qualify_dr-0.98_min-5_smooth.csv\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtickers \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_tickers_fname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#tickers selected:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtickers))\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_EOD_data()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TGC_torch-master/lib/python3.10/site-packages/numpy/lib/npyio.py:1980\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   1978\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[1;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1980\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1981\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TGC_torch-master/lib/python3.10/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TGC_torch-master/lib/python3.10/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /Users/liupeng/Desktop/DSAA 5020 Final Project/TGCN_with_latest_data/2023.12.14(Data acquisition& Preprocessing)/sp500_tickers_478.csv not found."
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class EOD_Preprocessor:\n",
    "    def __init__(self, data_path, market_name):\n",
    "        self.data_path = data_path\n",
    "        self.date_format = '%Y-%m-%d'\n",
    "        self.market_name = market_name\n",
    "\n",
    "    def _read_EOD_data(self):\n",
    "        self.data_EOD = []\n",
    "        for ticker in self.tickers:\n",
    "            file_path = os.path.join(self.data_path, \"S&P500_original_data\", f\"{ticker}_stock_data.csv\")\n",
    "            single_EOD = pd.read_csv(file_path)\n",
    "            self.data_EOD.append(single_EOD.values)\n",
    "        print('#stocks\\' EOD data readin:', len(self.data_EOD))\n",
    "        assert len(self.tickers) == len(self.data_EOD), 'length of tickers and stocks not match'\n",
    "\n",
    "    def _read_tickers(self):\n",
    "        tickers_path = os.path.join(self.data_path, \"Relation_Data(478)\", \"sp500_tickers_478.csv\")\n",
    "        self.tickers = pd.read_csv(tickers_path)['Ticker'].tolist()\n",
    "\n",
    "    \n",
    "    def _transfer_EOD_str(self, selected_EOD_str, tra_date_index):\n",
    "        selected_EOD = np.zeros(selected_EOD_str.shape, dtype=float)\n",
    "        for row, daily_EOD in enumerate(selected_EOD_str):\n",
    "            # print('daily_EOD:', daily_EOD)\n",
    "            date_str = daily_EOD[0]\n",
    "            # date_str = date_str.replace('-04:00', '')\n",
    "            # print('date_str:', date_str)\n",
    "            # print('tra_date_index:', tra_date_index)\n",
    "            selected_EOD[row][0] = tra_date_index[date_str]\n",
    "            # print('selected_EOD[row][0]:', selected_EOD[row][0])\n",
    "            for col in range(1, selected_EOD_str.shape[1]):\n",
    "                selected_EOD[row][col] = float(daily_EOD[col])\n",
    "        return selected_EOD\n",
    "\n",
    "    '''\n",
    "        Transform the original EOD data collected from Google Finance to a\n",
    "        friendly format to fit machine learning model via the following steps:\n",
    "            Calculate moving average (5-days, 10-days, 20-days, 30-days),\n",
    "            ignoring suspension days (market open, only suspend this stock)\n",
    "            Normalize features by (feature - min) / (max - min)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    def generate_feature(self, selected_tickers_fname, begin_date, opath,\n",
    "                         return_days=1, pad_begin=29):\n",
    "        # 新的交易日期文件路径\n",
    "        trading_dates_path = os.path.join(self.data_path, 'S&P500_aver_line_dates.csv')\n",
    "        trading_dates_df = pd.read_csv(trading_dates_path, header=None)\n",
    "        trading_dates = trading_dates_df.iloc[:, 0].tolist()  # 假设日期数据在第一列\n",
    "\n",
    "        # trading_dates = np.genfromtxt(\n",
    "        #     os.path.join(self.data_path,self.market_name + '_aver_line_dates_latest.csv'),\n",
    "        #     dtype=str, delimiter=',', skip_header=False\n",
    "        # )\n",
    "        # trading_dates = trading_dates[29:]  # offset for the first 30-days average\n",
    "        print('#trading dates:', len(trading_dates)) # trading dates是什么？为什么文件中显示是股票ticker的数量？ trading_dates：1275\n",
    "        # begin_date = datetime.strptime(trading_dates[29], self.date_format)\n",
    "        print('begin date:', begin_date)\n",
    "        # transform the trading dates into a dictionary with index\n",
    "        index_tra_dates = {}\n",
    "        tra_dates_index = {}\n",
    "        for index, date in enumerate(trading_dates):\n",
    "            tra_dates_index[date] = index\n",
    "            index_tra_dates[index] = date\n",
    "        # read in tickers file: NASDAQ_tickers_qualify_dr-0.98_min-5_smooth.csv\n",
    "        self.tickers = np.genfromtxt(\n",
    "            os.path.join(self.data_path, selected_tickers_fname),\n",
    "            dtype=str, delimiter='\\t', skip_header=False\n",
    "        )\n",
    "        print('#tickers selected:', len(self.tickers))\n",
    "        self._read_EOD_data()\n",
    "        for stock_index, single_EOD in enumerate(self.data_EOD):\n",
    "            # select data within the begin_date\n",
    "            # begin_date_row = -1\n",
    "            for date_index, daily_EOD in enumerate(single_EOD):\n",
    "                date_str = daily_EOD[0]\n",
    "                # date_str = date_str.replace('-04:00', '') #这部分代码为什么在_transfer_EOD_str中又被执行了一遍？\n",
    "                cur_date = datetime.strptime(date_str, self.date_format)\n",
    "                # if cur_date > begin_date:\n",
    "                    # print('cur_date:', cur_date)\n",
    "                    # begin_date_row = date_index\n",
    "                    # print(single_EOD[date_index,:])\n",
    "                    # break\n",
    "            selected_EOD_str = single_EOD\n",
    "            selected_EOD = self._transfer_EOD_str(selected_EOD_str,\n",
    "                                                  tra_dates_index)\n",
    "            # Question：数据形状对不上：selected_EOD_str.shape = (1274, 6), selected_EOD.shape = (1274, 6)\n",
    "            # calculate moving average features\n",
    "            begin_date_row = 29\n",
    "            # for row in selected_EOD[:, 0]:\n",
    "            #     row = int(row)\n",
    "            #     if row >= pad_begin:   # offset for the first 30-days average\n",
    "            #         begin_date_row = row\n",
    "            #         break\n",
    "            mov_aver_features = np.zeros(\n",
    "                [selected_EOD.shape[0], 4], dtype=float\n",
    "            )   # 4 columns refers to 5-, 10-, 20-, 30-days average\n",
    "            for row in range(begin_date_row, selected_EOD.shape[0]):\n",
    "                date_index = selected_EOD[row][0]\n",
    "                aver_5 = 0.0\n",
    "                aver_10 = 0.0\n",
    "                aver_20 = 0.0\n",
    "                aver_30 = 0.0\n",
    "                count_5 = 0\n",
    "                count_10 = 0\n",
    "                count_20 = 0\n",
    "                count_30 = 0\n",
    "                for offset in range(30):\n",
    "                    date_gap = date_index - selected_EOD[row - offset][0]\n",
    "                    if date_gap < 5:\n",
    "                        count_5 += 1\n",
    "                        aver_5 += selected_EOD[row - offset][4]\n",
    "                    if date_gap < 10:\n",
    "                        count_10 += 1\n",
    "                        aver_10 += selected_EOD[row - offset][4]\n",
    "                    if date_gap < 20:\n",
    "                        count_20 += 1\n",
    "                        aver_20 += selected_EOD[row - offset][4]\n",
    "                    if date_gap < 30:\n",
    "                        count_30 += 1\n",
    "                        aver_30 += selected_EOD[row - offset][4]\n",
    "                mov_aver_features[row][0] = aver_5 / count_5\n",
    "                mov_aver_features[row][1] = aver_10 / count_10\n",
    "                mov_aver_features[row][2] = aver_20 / count_20\n",
    "                mov_aver_features[row][3] = aver_30 / count_30\n",
    "\n",
    "            '''\n",
    "                normalize features by feature / max, the max price is the\n",
    "                max of close prices, I give up to subtract min for easier\n",
    "                return ratio calculation.\n",
    "            '''\n",
    "            pri_min = np.min(selected_EOD[:, 5]) # 收盘价的最小值\n",
    "            price_max = np.max(selected_EOD[:, 5])# 收盘价的最大值\n",
    "            print(self.tickers[stock_index], 'minimum:', pri_min,\n",
    "                  'maximum:', price_max, 'ratio:', price_max / pri_min)\n",
    "            if price_max / pri_min > 10:      # 价格最大值与最小值的比值大于10，说明这个股票的价格波动很大，不适合用来做预测 ?\n",
    "                print('!!!!!!!!!')\n",
    "            # open_high_low = (selected_EOD[:, 1:4] - price_min) / \\\n",
    "            #                 (price_max - price_min)\n",
    "            mov_aver_features = mov_aver_features / price_max  #移动平均值除以收盘价最大值进行标准化是否合理？\n",
    "\n",
    "            '''\n",
    "                generate feature and ground truth in the following format:\n",
    "                date_index, 5-day, 10-day, 20-day, 30-day, close price\n",
    "                two ways to pad missing dates:\n",
    "                for dates without record, pad a row [date_index, -1234 * 5]\n",
    "            '''\n",
    "            features = np.ones([len(trading_dates) - pad_begin, 6],\n",
    "                               dtype=float) * -1234\n",
    "            # features：(1275-29=1246, 6)\n",
    "            # data missed at the beginning\n",
    "            for row in range(len(trading_dates) - pad_begin):\n",
    "                features[row][0] = row\n",
    "            for row in range(begin_date_row, selected_EOD.shape[0]):\n",
    "                cur_index = int(selected_EOD[row][0])\n",
    "                features[cur_index - pad_begin][1:5] = mov_aver_features[row]\n",
    "                # 这个if语句的作用是什么？有什么意义？\n",
    "                # if cur_index - int(selected_EOD[row - return_days][0]) == return_days:\n",
    "                features[cur_index - pad_begin][-1] = selected_EOD[row][4] / price_max\n",
    "\n",
    "            # write out\n",
    "            np.savetxt(os.path.join(opath, self.market_name + '_' +\n",
    "                                    self.tickers[stock_index] + '_' +\n",
    "                                    str(return_days) + '.csv'), features,\n",
    "                       fmt='%.6f', delimiter=',')\n",
    "\n",
    "# 设置数据路径和市场名称\n",
    "data_path = '/Users/liupeng/Desktop/DSAA 5020 Final Project/TGCN_with_latest_data/2023.12.14(Data acquisition& Preprocessing)'\n",
    "market_name = 'S&P500'\n",
    "\n",
    "# 创建EOD_Preprocessor实例\n",
    "processor = EOD_Preprocessor(data_path, market_name)\n",
    "\n",
    "# 调用generate_feature方法处理数据\n",
    "processor.generate_feature(\n",
    "    'sp500_tickers_478.csv',\n",
    "    datetime.strptime('2015-01-02', '%Y-%m-%d'),\n",
    "    os.path.join(data_path, 'SP500_EOD'),\n",
    "    return_days=1,\n",
    "    pad_begin=29\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TGC_torch-master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
